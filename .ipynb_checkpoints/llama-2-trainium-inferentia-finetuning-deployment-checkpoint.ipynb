{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfa4226-79a1-4520-b131-c798e42e34f7",
   "metadata": {},
   "source": [
    "# Fine-tune and deploy LLaMA V2 models on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) and [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081de5c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29838a99",
   "metadata": {},
   "source": [
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 2 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) and [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances.\n",
    "\n",
    "AWS Neuron is an SDK with a compiler, runtime, and profiling tools that unlocks high-performance and cost-effective deep learning (DL) acceleration. It supports high-performance training on AWS Trainium-based Amazon Elastic Compute Cloud (Amazon EC2) Trn1 instances. For model deployment, it supports high-performance and low-latency inference on AWS Inferentia-based Amazon EC2 Inf1 instances and AWS Inferentia2-based Amazon EC2 Inf2 instances. For details, see [Official documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9b99d-639b-40f3-91e3-1fe00ee032a4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass 'accept_eula=True' as part of model.deploy() call. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets 'accept_eula=False', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Similarly, to perform fine-tuning on these models, you need pass environment variable '{\"accept_eula\": \"true\"}' to `JumpStartEstimator` class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Set up\n",
    "\n",
    "---\n",
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.199.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.213.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.8.2)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2023.11.17)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker) (0.58.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Collecting urllib3<3.0.0,>=1.26.8 (from sagemaker)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Using cached sagemaker-2.213.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: xxhash, urllib3, pyarrow-hotfix, multidict, fsspec, frozenlist, async-timeout, yarl, aiosignal, huggingface-hub, aiohttp, sagemaker, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.199.0\n",
      "    Uninstalling sagemaker-2.199.0:\n",
      "      Successfully uninstalled sagemaker-2.199.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.21.4 multidict-6.0.5 pyarrow-hotfix-0.6 sagemaker-2.213.0 urllib3-2.0.7 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart through 2 options. Option 1 allows you to quickly deploy the endpoint with default setting in two lines of code. Option 2 allows you to have more customized configurations. \n",
    "\n",
    "To deploy a model on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) or [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances, we will firstly need call PyTorch Neuron ([torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html?highlight=graph#neuron-persistent-cache)) to compile the model into a Neuron specific graph. Then during runtime the graph is executed on the **NeuronCores** of the [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) or [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances. Compiling the graph involves running optimizations that can make use of the NeuronCores efficiently.\n",
    "\n",
    "In SageMaker JumpStart, we pre-compile the neuron graphs for a varieity of configurations such that you do not spend time waiting for compiling the graph during endpoint deployment, as long as the deployment parameters (**environmental variables**) matches one of configurations listed as below. Otherwise, the compilation will be triggered during endpoint deployment, which will take a slightly longer time to deploy a model.\n",
    "\n",
    "|||LLaMA V2 7B and 7B Chat|||\n",
    "|---|---|---|---|---|\n",
    "|Instance type|Context length|Batch size| Tensor parallel degree| Data type |\n",
    "|ml.inf2.xlarge|1024|1|2|fp16|\n",
    "|ml.inf2.8xlarge|2048|1|2|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|4|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|4|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|24|fp16|\n",
    "\n",
    "\n",
    "|||LLaMA V2 13B and 13B Chat|||\n",
    "|---|---|---|---|---|\n",
    "|Instance type|Context length|Batch size| Tensor parallel degree| Data type |\n",
    "|ml.inf2.8xlarge|1024|1|2|fp16|\n",
    "|ml.inf2.24xlarge|2048|4|4|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|2048|4|4|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|24|fp16|\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e01401-82db-4d49-9457-f930f4138618",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgenerationneuron-llama-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89769446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"1.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "971c9087",
   "metadata": {
    "jumpStartAlterations": [
     "dropModelSelection"
    ],
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53382a36b4db49759c119406b33347ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select a model', index=4, layout=Layout(width='max-content'), options=('meta-textgeneratâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And\n",
    "\n",
    "\n",
    "filter_value = And(\"task == textgenerationneuron\", \"framework == meta\")\n",
    "model_ids_meta_textgenerationneuron = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=model_ids_meta_textgenerationneuron,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b862f0",
   "metadata": {
    "jumpStartAlterations": [
     "dropModelSelection"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = model_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e648c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXAMPLE_ENV = {\n",
    "    \"meta-textgenerationneuron-llama-2-13b-f\": {\n",
    "        \"context_length\": \"1024\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "    \"meta-textgenerationneuron-llama-2-13b\": {\n",
    "        \"context_length\": \"1024\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "    \"meta-textgenerationneuron-llama-2-7b-f\": {\n",
    "        \"context_length\": \"2048\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "    \"meta-textgenerationneuron-llama-2-7b\": {\n",
    "        \"context_length\": \"2048\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "option = \"option1\"\n",
    "\n",
    "if option == \"option1\":\n",
    "    model = JumpStartModel(model_id=model_id)\n",
    "\n",
    "else:\n",
    "    model = JumpStartModel(\n",
    "        model_id=model_id,\n",
    "        env={\n",
    "            \"OPTION_DTYPE\": \"fp16\",  ## correspond to the column `Data type`\n",
    "            \"OPTION_N_POSITIONS\": EXAMPLE_ENV[model_id][\n",
    "                \"context_length\"\n",
    "            ],  ## correspond to the column `Contexnt length`\n",
    "            \"OPTION_TENSOR_PARALLEL_DEGREE\": EXAMPLE_ENV[model_id][\n",
    "                \"tensor_parallel_degree\"\n",
    "            ],  ## correspond to the column `Tensor parallel degree`\n",
    "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": EXAMPLE_ENV[model_id][\n",
    "                \"batch_size\"\n",
    "            ],  ## correspond to the column `Batch size`\n",
    "        },\n",
    "        instance_type=EXAMPLE_ENV[model_id][\n",
    "            \"instance_type\"\n",
    "        ],  ## correspond to the column `Instance type`\n",
    "    )\n",
    "\n",
    "pretrained_predictor = model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response['generated_text']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy. I believe that happiness is a choice. I believe that happiness is a state of mind. I believe that happiness is a state of being. I believe that happiness is a state of being. I believe that happiness is a state of being. I believe that happiness is a state of being. I believe\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload)\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train directory containing either a JSON lines (`.jsonl`) or text (`.txt`) formatted file. \n",
    "  - For JSON lines (JSONL) file, each line is a dictionary, repsentating a dictionary. The key in dictionary (each line) has to be 'text'.\n",
    "  - The number of files under train directory should equal to one. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "For demonstration of using text file as input, please see [Appendix 2](#2.-Use-text-file-as-input-to-fine-tune-LLaMA-2)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0148f2488a604516a1207d19556574a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f1706fa6e4469a9f4168542be5f683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2233560"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "task = \"information_extraction\"\n",
    "# To train for summarization/closed question and answering, you can replace the assertion in next line to example[\"category\"] == \"sumarization\"/\"closed_qa\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == task)\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'If the year is 2020, is the Samsung E1200 still in production?',\n",
       " 'context': 'The Samsung E1200 (also known as Samsung Pusha and Samsung Keystone 2) is a mobile phone made by Samsung. This phone was released in 2012 through Tesco in the UK. It was discontinued in 2015 when the Samsung Guru FM E1202 was released as its successor.',\n",
       " 'response': 'No, the phone was discontinued in 2015.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we use a prompt template for preprocessing the data in an instruction / input format for the training job, and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}### Response:\\n{response}\\n\\n<s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a19a34-1afd-4483-ac9f-81574d03d9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            instruction=sample[\"instruction\"],\n",
    "            context=sample[\"context\"],\n",
    "            response=sample[\"response\"],\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b5af455-3426-4d2e-9377-a57daa2cdd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb36c9669b54df98ce95e0ff7068051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a889a70e201045709fc1c407b2daa407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_processed = train_and_test_dataset.map(\n",
    "    apply_prompt_template, remove_columns=list(train_and_test_dataset[\"train\"].features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1804128d-c3c8-410b-9ab6-b1e0a423c20b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a3096fbd1247b6b3413937f125fd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ea643eae554c5fa0f193ddca2c4cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "288204"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed[\"train\"].to_json(f\"dolly/processed-train-{task}.jsonl\")\n",
    "dataset_processed[\"test\"].to_json(f\"dolly/processed-test-{task}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-2-850751315356/dolly_dataset_trn1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = f\"dolly/processed-train-{task}.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset_trn1\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 model on the summarization dataset from Dolly on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) instance. You have two options: `ml.trn1.32xlarge` (default) and `ml.trn1n.32xlarge`. Finetuning scripts are based on scripts provided by [Neuronx-Nemo-Megatron](https://github.com/aws-neuron/neuronx-nemo-megatron). For a list of supported hyper-parameters and their default values, please see [supported hyperparameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc9a6b0-b4df-4420-b55d-1906e215b79e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_input_length': '2048', 'preprocessing_num_workers': 'None', 'learning_rate': '6e-06', 'min_learning_rate': '1e-06', 'max_steps': '20', 'global_train_batch_size': '256', 'per_device_train_batch_size': '1', 'layer_norm_epilson': '1e-05', 'weight_decay': '0.1', 'lr_scheduler_type': 'CosineAnnealing', 'warmup_steps': '10', 'constant_steps': '0', 'adam_beta1': '0.9', 'adam_beta2': '0.95', 'mixed_precision': 'True', 'tensor_parallel_degree': '8', 'pipeline_parallel_degree': '1', 'append_eod': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc46b1-bf7d-4ba3-9bff-4015189ea43e",
   "metadata": {},
   "source": [
    "Overwrite some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ee59a63-656d-49cb-b72d-08ebd8afb1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_input_length': '2048', 'preprocessing_num_workers': 'None', 'learning_rate': '0.01', 'min_learning_rate': '1e-06', 'max_steps': '10', 'global_train_batch_size': '256', 'per_device_train_batch_size': '1', 'layer_norm_epilson': '1e-05', 'weight_decay': '0.1', 'lr_scheduler_type': 'CosineAnnealing', 'warmup_steps': '10', 'constant_steps': '0', 'adam_beta1': '0.9', 'adam_beta2': '0.95', 'mixed_precision': 'True', 'tensor_parallel_degree': '8', 'pipeline_parallel_degree': '1', 'append_eod': 'False'}\n"
     ]
    }
   ],
   "source": [
    "# my_hyperparameters[\"max_input_length\"] = \"4096\" # you can increase it up to 4096 for sequence length.\n",
    "my_hyperparameters[\"max_steps\"] = \"10\"\n",
    "my_hyperparameters[\"learning_rate\"] = \"0.01\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5a3e2-5b64-4fca-aec7-045ccfa4c24b",
   "metadata": {},
   "source": [
    "Validate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6685ff5b-5f63-4822-85c0-1a65cca0e713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgenerationneuron-llama-2-7b-2024-03-18-21-26-15-153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-18 21:26:15 Starting - Starting the training job...\n",
      "2024-03-18 21:26:32 Starting - Preparing the instances for training......\n",
      "2024-03-18 21:27:22 Downloading - Downloading input data........................\n",
      "2024-03-18 21:31:49 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:49,583 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:49,583 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:49,617 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:49,626 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:49,628 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:51,161 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,273 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,273 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,274 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,309 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,319 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,352 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,364 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,396 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,406 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.trn1.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.95\",\n",
      "        \"append_eod\": \"False\",\n",
      "        \"constant_steps\": \"0\",\n",
      "        \"global_train_batch_size\": \"256\",\n",
      "        \"layer_norm_epilson\": \"1e-05\",\n",
      "        \"learning_rate\": \"0.01\",\n",
      "        \"lr_scheduler_type\": \"CosineAnnealing\",\n",
      "        \"max_input_length\": \"2048\",\n",
      "        \"max_steps\": \"10\",\n",
      "        \"min_learning_rate\": \"1e-06\",\n",
      "        \"mixed_precision\": \"True\",\n",
      "        \"per_device_train_batch_size\": \"1\",\n",
      "        \"pipeline_parallel_degree\": \"1\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"tensor_parallel_degree\": \"8\",\n",
      "        \"warmup_steps\": \"10\",\n",
      "        \"weight_decay\": \"0.1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.trn1.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"meta-textgenerationneuron-llama-2-7b-2024-03-18-21-26-15-153\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 2,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.trn1.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.trn1.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.95\",\"append_eod\":\"False\",\"constant_steps\":\"0\",\"global_train_batch_size\":\"256\",\"layer_norm_epilson\":\"1e-05\",\"learning_rate\":\"0.01\",\"lr_scheduler_type\":\"CosineAnnealing\",\"max_input_length\":\"2048\",\"max_steps\":\"10\",\"min_learning_rate\":\"1e-06\",\"mixed_precision\":\"True\",\"per_device_train_batch_size\":\"1\",\"pipeline_parallel_degree\":\"1\",\"preprocessing_num_workers\":\"None\",\"tensor_parallel_degree\":\"8\",\"warmup_steps\":\"10\",\"weight_decay\":\"0.1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.trn1.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=2\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.trn1.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.95\",\"append_eod\":\"False\",\"constant_steps\":\"0\",\"global_train_batch_size\":\"256\",\"layer_norm_epilson\":\"1e-05\",\"learning_rate\":\"0.01\",\"lr_scheduler_type\":\"CosineAnnealing\",\"max_input_length\":\"2048\",\"max_steps\":\"10\",\"min_learning_rate\":\"1e-06\",\"mixed_precision\":\"True\",\"per_device_train_batch_size\":\"1\",\"pipeline_parallel_degree\":\"1\",\"preprocessing_num_workers\":\"None\",\"tensor_parallel_degree\":\"8\",\"warmup_steps\":\"10\",\"weight_decay\":\"0.1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"meta-textgenerationneuron-llama-2-7b-2024-03-18-21-26-15-153\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":2,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.95\",\"--append_eod\",\"False\",\"--constant_steps\",\"0\",\"--global_train_batch_size\",\"256\",\"--layer_norm_epilson\",\"1e-05\",\"--learning_rate\",\"0.01\",\"--lr_scheduler_type\",\"CosineAnnealing\",\"--max_input_length\",\"2048\",\"--max_steps\",\"10\",\"--min_learning_rate\",\"1e-06\",\"--mixed_precision\",\"True\",\"--per_device_train_batch_size\",\"1\",\"--pipeline_parallel_degree\",\"1\",\"--preprocessing_num_workers\",\"None\",\"--tensor_parallel_degree\",\"8\",\"--warmup_steps\",\"10\",\"--weight_decay\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.95\u001b[0m\n",
      "\u001b[34mSM_HP_APPEND_EOD=False\u001b[0m\n",
      "\u001b[34mSM_HP_CONSTANT_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_GLOBAL_TRAIN_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LAYER_NORM_EPILSON=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=CosineAnnealing\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=2048\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LEARNING_RATE=1e-06\u001b[0m\n",
      "\u001b[34mSM_HP_MIXED_PRECISION=True\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_PARALLEL_DEGREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_TENSOR_PARALLEL_DEGREE=8\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python310.zip:/usr/local/lib/python3.10:/usr/local/lib/python3.10/lib-dynload:/usr/local/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.95 --append_eod False --constant_steps 0 --global_train_batch_size 256 --layer_norm_epilson 1e-05 --learning_rate 0.01 --lr_scheduler_type CosineAnnealing --max_input_length 2048 --max_steps 10 --min_learning_rate 1e-06 --mixed_precision True --per_device_train_batch_size 1 --pipeline_parallel_degree 1 --preprocessing_num_workers None --tensor_parallel_degree 8 --warmup_steps 10 --weight_decay 0.1\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,407 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:31:53,407 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers_neuronx/transformers_neuronx-0.8.268-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/site-packages (from transformers-neuronx==0.8.268) (0.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch-neuronx in /usr/local/lib/python3.10/site-packages (from transformers-neuronx==0.8.268) (1.13.1.1.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (from transformers-neuronx==0.8.268) (4.33.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (0.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch-xla==1.13.1+torchneuronb in /usr/local/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx==0.8.268) (1.13.1+torchneuronb)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: libneuronxla==0.5.476 in /usr/local/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx==0.8.268) (0.5.476)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5 in /usr/local/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx==0.8.268) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aws-neuronx-runtime-discovery~=2.0 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: neuronx-cc~=2.0 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.10.0.35+3817a0c8c)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3~=1.26 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.28.57)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore~=1.29 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.31.57)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (2023.8.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from huggingface-hub->accelerate->transformers-neuronx==0.8.268) (2023.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/site-packages (from botocore~=1.29->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauth2client in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (4.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.35.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.34.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: neuronx-hwm==2.10.0.5 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.10.0.5+7b1976adf)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx<=2.6.3 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<=1.7.3 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-daemon>=2.2.4 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-unixsocket>=0.1.5 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: islpy<=2023.1,>2021.1 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2023.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pgzip>=0.3.0 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ec2-metadata<=2.10.0 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils in /usr/local/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.60.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (4.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (3.1.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: transformers-neuronx\u001b[0m\n",
      "\u001b[34mSuccessfully installed transformers-neuronx-0.8.268\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/Cython/Cython-3.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: Cython\u001b[0m\n",
      "\u001b[34mSuccessfully installed Cython-3.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/soupsieve/soupsieve-2.5-py3-none-any.whl (from -r extra_requirement.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sentence-transformers/sentence-transformers-2.2.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyarrow/pyarrow-13.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/omegaconf/omegaconf-2.2.3-py3-none-any.whl (from -r extra_requirement.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/portalocker/portalocker-2.8.2-py3-none-any.whl (from -r extra_requirement.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/einops/einops-0.7.0-py3-none-any.whl (from -r extra_requirement.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sacrebleu/sacrebleu-2.3.1-py3-none-any.whl (from -r extra_requirement.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ftfy/ftfy-6.1.1-py3-none-any.whl (from -r extra_requirement.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic/pydantic-2.4.2-py3-none-any.whl (from -r extra_requirement.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ijson/ijson-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/OpenCC/OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (from -r extra_requirement.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/GPUtil/GPUtil-1.4.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/faiss_cpu/faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/smmap/smmap-5.0.1-py3-none-any.whl (from -r extra_requirement.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/aiohttp/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nltk/nltk-3.8.1-py3-none-any.whl (from -r extra_requirement.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/rapidfuzz/rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/frozenlist/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathtools/pathtools-0.1.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ipadic/ipadic-1.0.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightning_utilities/lightning_utilities-0.9.0-py3-none-any.whl (from -r extra_requirement.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/async_timeout/async_timeout-4.0.3-py3-none-any.whl (from -r extra_requirement.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/braceexpand/braceexpand-0.1.7-py2.py3-none-any.whl (from -r extra_requirement.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/yarl/yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lxml/lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from -r extra_requirement.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/antlr4-python3-runtime/antlr4-python3-runtime-4.9.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mecab_python3/mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/hydra_core/hydra_core-1.2.0-py3-none-any.whl (from -r extra_requirement.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflect/inflect-7.0.0-py3-none-any.whl (from -r extra_requirement.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/setproctitle/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.5-py3-none-any.whl (from -r extra_requirement.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tensorboardX/tensorboardX-2.6.2.2-py2.py3-none-any.whl (from -r extra_requirement.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sacremoses/sacremoses-0.0.53.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/PyYAML/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/GitPython/GitPython-3.1.37-py3-none-any.whl (from -r extra_requirement.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sentry_sdk/sentry_sdk-1.32.0-py2.py3-none-any.whl (from -r extra_requirement.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dill/dill-0.3.7-py3-none-any.whl (from -r extra_requirement.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multidict/multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/appdirs/appdirs-1.4.4-py2.py3-none-any.whl (from -r extra_requirement.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docker_pycreds/docker_pycreds-0.4.0-py2.py3-none-any.whl (from -r extra_requirement.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic_core/pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/xxhash/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/gdown/gdown-4.7.1-py3-none-any.whl (from -r extra_requirement.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/jieba/jieba-0.42.1.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybind11/pybind11-2.11.1-py3-none-any.whl (from -r extra_requirement.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r extra_requirement.txt (line 46))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torchmetrics/torchmetrics-0.10.3-py3-none-any.whl (from -r extra_requirement.txt (line 47))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch_lightning/pytorch_lightning-1.8.6-py3-none-any.whl (from -r extra_requirement.txt (line 48))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/beautifulsoup4/beautifulsoup4-4.12.2-py3-none-any.whl (from -r extra_requirement.txt (line 49))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/youtokentome/youtokentome-1.0.6.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tabulate/tabulate-0.9.0-py3-none-any.whl (from -r extra_requirement.txt (line 51))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/aiosignal/aiosignal-1.3.1-py3-none-any.whl (from -r extra_requirement.txt (line 52))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/PySocks/PySocks-1.7.1-py3-none-any.whl (from -r extra_requirement.txt (line 53))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/annotated_types/annotated_types-0.6.0-py3-none-any.whl (from -r extra_requirement.txt (line 54))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pangu/pangu-4.0.6.1-py3-none-any.whl (from -r extra_requirement.txt (line 55))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/gitdb/gitdb-4.0.10-py3-none-any.whl (from -r extra_requirement.txt (line 56))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wandb/wandb-0.15.12-py3-none-any.whl (from -r extra_requirement.txt (line 57))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fsspec/fsspec-2023.6.0-py3-none-any.whl (from -r extra_requirement.txt (line 58))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/webdataset/webdataset-0.1.62-py3-none-any.whl (from -r extra_requirement.txt (line 59))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wcwidth/wcwidth-0.2.8-py2.py3-none-any.whl (from -r extra_requirement.txt (line 60))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from sacrebleu==2.3.1->-r extra_requirement.txt (line 7)) (2023.8.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /usr/local/lib/python3.10/site-packages (from sacrebleu==2.3.1->-r extra_requirement.txt (line 7)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/site-packages (from pydantic==2.4.2->-r extra_requirement.txt (line 9)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.6->-r extra_requirement.txt (line 15)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.6->-r extra_requirement.txt (line 15)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1->-r extra_requirement.txt (line 16)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1->-r extra_requirement.txt (line 16)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/site-packages (from lightning-utilities==0.9.0->-r extra_requirement.txt (line 21)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/site-packages (from yarl==1.9.2->-r extra_requirement.txt (line 24)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.14.5->-r extra_requirement.txt (line 31)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.14.5->-r extra_requirement.txt (line 31)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets==2.14.5->-r extra_requirement.txt (line 31)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/site-packages (from tensorboardX==2.6.2.2->-r extra_requirement.txt (line 32)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from sacremoses==0.0.53->-r extra_requirement.txt (line 33)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from sentry-sdk==1.32.0->-r extra_requirement.txt (line 36)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.10/site-packages (from sentry-sdk==1.32.0->-r extra_requirement.txt (line 36)) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown==4.7.1->-r extra_requirement.txt (line 43)) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.31.0->-r extra_requirement.txt (line 46)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.31.0->-r extra_requirement.txt (line 46)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb==0.15.12->-r extra_requirement.txt (line 57)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb==0.15.12->-r extra_requirement.txt (line 57)) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.14.5->-r extra_requirement.txt (line 31)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.14.5->-r extra_requirement.txt (line 31)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.14.5->-r extra_requirement.txt (line 31)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (10.0.1)\u001b[0m\n",
      "\u001b[34mPyYAML is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mdill is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sentence-transformers, antlr4-python3-runtime, GPUtil, pathtools, ipadic, sacremoses, jieba, youtokentome\u001b[0m\n",
      "\u001b[34mBuilding wheel for sentence-transformers (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sentence-transformers (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=092f77faaedc83fd21337765862249a2c65095190f7e91112d5c4a8a34b08c19\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e6/4d/bd/2759d6378cdae8c5bf306a568a08ca046e22f4a783b920c77a\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a09df4baa8be44801bb11868209f30d6d97d2afe597172650bbc9d84f706557f\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/fe/0a/e5/208460ef4014a049ccce20703f142336caa9e4ecf1391c7ccf\u001b[0m\n",
      "\u001b[34mBuilding wheel for GPUtil (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for GPUtil (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=77ba9642021a4035de859274d22e5c000d98753af45a5c2fe3b39b551c8845ab\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7e/0c/c1/5539f46eefa0b6fd0d7bc28e14c8bc396f84f2d68b2864dfbd\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=8c5804259581ad7cf9dcc46fc23833561d17f8f8265ebf0f2d5424e9929da44e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/07/68/48/289b55053de539a27e3d2588a5c5dc0d269353b7b749a24474\u001b[0m\n",
      "\u001b[34mBuilding wheel for ipadic (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ipadic (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556703 sha256=724879e573b823f9e4338069b1b7e8bf17dda4e8e0d2e9498878168c72ad0bea\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/07/4e/dc/9c0933c272e2186098f1e25735168913d25a1142f68b5edd1a\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=1b827bdff8a75e0a8e6edcdab70b5dcc3e541c81ab82efcd8c1702a19eee6df5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/12/5f/2e/8f4084e4d365d4e0fdececab575df828dacca9804451a79a11\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=65a2d6e492e6231ddbc939a37c70d971b8b5ee934484d0d2fef589906c214d6e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/fe/c1/04/36aebeeee83a5f6ac34f8f3f47fdabba32c1a84437756065be\u001b[0m\n",
      "\u001b[34mBuilding wheel for youtokentome (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for youtokentome (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1984754 sha256=b2c324562004390c8e0f65cb1a9f33a952cb1a0d664ac156851da58f3541aa85\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5d/03/1c/07fbb33096f7d93a0266c39d6dfe0ad92339e3ec69f02969fa\u001b[0m\n",
      "\u001b[34mSuccessfully built sentence-transformers antlr4-python3-runtime GPUtil pathtools ipadic sacremoses jieba youtokentome\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wcwidth, pathtools, pangu, OpenCC, mecab-python3, jieba, ipadic, ijson, GPUtil, faiss-cpu, braceexpand, appdirs, antlr4-python3-runtime, youtokentome, xxhash, webdataset, tensorboardX, tabulate, soupsieve, smmap, setproctitle, sentry-sdk, sacremoses, rapidfuzz, PySocks, pydantic-core, pybind11, pyarrow, portalocker, nltk, multidict, lxml, lightning-utilities, ftfy, fsspec, frozenlist, einops, docker-pycreds, async-timeout, annotated-types, yarl, sacrebleu, pydantic, omegaconf, gitdb, beautifulsoup4, aiosignal, transformers, inflect, hydra-core, GitPython, gdown, aiohttp, wandb, torchmetrics, sentence-transformers, pytorch-lightning, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.9.2\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.9.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.9.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.33.3\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.33.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.33.3\u001b[0m\n",
      "\u001b[34mSuccessfully installed GPUtil-1.4.0 GitPython-3.1.37 OpenCC-1.1.6 PySocks-1.7.1 aiohttp-3.8.6 aiosignal-1.3.1 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 async-timeout-4.0.3 beautifulsoup4-4.12.2 braceexpand-0.1.7 datasets-2.14.5 docker-pycreds-0.4.0 einops-0.7.0 faiss-cpu-1.7.4 frozenlist-1.4.0 fsspec-2023.6.0 ftfy-6.1.1 gdown-4.7.1 gitdb-4.0.10 hydra-core-1.2.0 ijson-3.2.3 inflect-7.0.0 ipadic-1.0.0 jieba-0.42.1 lightning-utilities-0.9.0 lxml-4.9.3 mecab-python3-1.0.8 multidict-6.0.4 nltk-3.8.1 omegaconf-2.2.3 pangu-4.0.6.1 pathtools-0.1.2 portalocker-2.8.2 pyarrow-13.0.0 pybind11-2.11.1 pydantic-2.4.2 pydantic-core-2.10.1 pytorch-lightning-1.8.6 rapidfuzz-3.4.0 sacrebleu-2.3.1 sacremoses-0.0.53 sentence-transformers-2.2.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 soupsieve-2.5 tabulate-0.9.0 tensorboardX-2.6.2.2 torchmetrics-0.10.3 transformers-4.31.0 wandb-0.15.12 wcwidth-0.2.8 webdataset-0.1.62 xxhash-3.4.1 yarl-1.9.2 youtokentome-1.0.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/onnx/onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from onnx==1.14.1) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/site-packages (from onnx==1.14.1) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/site-packages (from onnx==1.14.1) (4.8.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: onnx\u001b[0m\n",
      "\u001b[34mSuccessfully installed onnx-1.14.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ruamel.yaml.clib/ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ruamel.yaml.clib\u001b[0m\n",
      "\u001b[34mSuccessfully installed ruamel.yaml.clib-0.2.8\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ruamel.yaml/ruamel.yaml-0.17.35-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/site-packages (from ruamel.yaml==0.17.35) (0.2.8)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ruamel.yaml\u001b[0m\n",
      "\u001b[34mSuccessfully installed ruamel.yaml-0.17.35\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/setuptools/setuptools-59.5.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: setuptools\u001b[0m\n",
      "\u001b[34mAttempting uninstall: setuptools\u001b[0m\n",
      "\u001b[34mFound existing installation: setuptools 68.2.2\u001b[0m\n",
      "\u001b[34mUninstalling setuptools-68.2.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled setuptools-68.2.2\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpython-daemon 3.0.1 requires setuptools>=62.4.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed setuptools-59.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/text_unidecode/text_unidecode-1.3-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: text-unidecode\u001b[0m\n",
      "\u001b[34mSuccessfully installed text-unidecode-1.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wget/wget-3.2.zip\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: wget\u001b[0m\n",
      "\u001b[34mBuilding wheel for wget (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for wget (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=db1d9b2af09ac0e2f9688a5308710c07e69a9fe9893312ad3f951acf48e39812\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/51/ff/3e/aa0659a53eabadfc50e4ee9ccb3297c417e59743edba43d42f\u001b[0m\n",
      "\u001b[34mSuccessfully built wget\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wget\u001b[0m\n",
      "\u001b[34mSuccessfully installed wget-3.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wrapt/wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wrapt\u001b[0m\n",
      "\u001b[34mSuccessfully installed wrapt-1.15.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./build/apex-0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./build/nemo_toolkit-1.14.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/site-packages (from apex==0.1) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (0.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (0.56.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (0.17.35)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (59.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (2.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: text-unidecode in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wget in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch-lightning==1.8.6 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.8.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (2.6.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/site-packages (from onnx>=1.7.0->nemo-toolkit==1.14.0) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->nemo-toolkit==1.14.0) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub->nemo-toolkit==1.14.0) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub->nemo-toolkit==1.14.0) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/site-packages (from numba->nemo-toolkit==1.14.0) (0.39.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil->nemo-toolkit==1.14.0) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/site-packages (from ruamel.yaml->nemo-toolkit==1.14.0) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->nemo-toolkit==1.14.0) (1.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn->nemo-toolkit==1.14.0) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->nemo-toolkit==1.14.0) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (1.59.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (1.35.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (0.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (3.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (2.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (3.8.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (4.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nemo-toolkit==1.14.0) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard->nemo-toolkit==1.14.0) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nemo-toolkit==1.14.0) (3.2.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: apex, nemo-toolkit\u001b[0m\n",
      "\u001b[34mSuccessfully installed apex-0.1 nemo-toolkit-1.14.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/numpy/numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.21.6\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.21.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.21.6\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mneuronx-cc 2.10.0.35+3817a0c8c requires numpy<=1.21.6,>=1.20, but you have numpy 1.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.22.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightning_fabric/lightning_fabric-2.0.9-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (1.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (3.8.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->lightning-fabric==2.0.9) (59.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->lightning-fabric==2.0.9) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (2023.7.22)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: lightning-fabric\u001b[0m\n",
      "\u001b[34mSuccessfully installed lightning-fabric-2.0.9\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m/opt/ml/code/nemo/nemo/collections/nlp/data/language_modeling/megatron /opt/ml/code\u001b[0m\n",
      "\u001b[34mg++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/local/include/python3.10 -I/usr/local/lib/python3.10/site-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so\u001b[0m\n",
      "\u001b[34m/opt/ml/code\u001b[0m\n",
      "\u001b[34m{'architectures': ['LlamaForCausalLM'], 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 11008, 'max_position_embeddings': 2048, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 32, 'pad_token_id': 0, 'rms_norm_eps': 1e-05, 'tie_word_embeddings': False, 'torch_dtype': 'float16', 'transformers_version': '4.28.1', 'use_cache': True, 'vocab_size': 32000}\u001b[0m\n",
      "\u001b[34mLoading /opt/ml/additonals3data/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mLoading /opt/ml/additonals3data/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34m323\u001b[0m\n",
      "\u001b[34mLoaded Llama model\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 0 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 1 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 2 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 3 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 4 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 5 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 6 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 7 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mDone saving Megatron checkpoint\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:35:35 optimizers:67] Could not import distributed_fused_adam optimizer from Apex\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:35:38 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:35:39 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mVocab size: 32000\u001b[0m\n",
      "\u001b[34mOutput prefix: tmp/tokenized_data\u001b[0m\n",
      "\u001b[34mTime to startup: 0.014322519302368164\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mProcessing file /opt/ml/input/data/train/processed-train-information_extraction.jsonl 1/1\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:35:40 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mProcessed 100 documents\u001b[0m\n",
      "\u001b[34m(609.6212599088977 docs/s, 0.9173713478426436 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 200 documents\u001b[0m\n",
      "\u001b[34m(1043.382563291218 docs/s, 1.703082904841153 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 300 documents\u001b[0m\n",
      "\u001b[34m(977.1997048887508 docs/s, 1.6617372733273794 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 400 documents (1140.3995459396263 docs/s, 2.0018733389071284 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 500 documents (1311.825212741735 docs/s, 2.2690666886852173 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 600 documents (1445.5108778814497 docs/s, 2.483499946006925 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 700 documents (1495.7457528596588 docs/s, 2.561903786675796 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 800 documents (1555.1933584634216 docs/s, 2.6744346541229427 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 900 documents (1619.4870065601074 docs/s, 2.7993600774290868 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1000 documents (1711.8762706005132 docs/s, 2.928660258003584 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1100 documents (1670.0212836810633 docs/s, 2.856197605223913 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1200 documents (1668.306235707744 docs/s, 2.9024913065628613 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1300 documents (1779.7006368948594 docs/s, 3.1116503159015476 MB/s).\u001b[0m\n",
      "\u001b[34m2024-03-18 21:35:41.000709:  274  INFO ||NEURON_PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\u001b[0m\n",
      "\u001b[34m./fine_tuning_trn1.sh: line 6:\u001b[0m\n",
      "\u001b[34msudo: command not found\u001b[0m\n",
      "\u001b[34m--nproc_per\u001b[0m\n",
      "\u001b[34m_node 32 --nnodes\u001b[0m\n",
      "\u001b[34m1 --node_rank 0 --master_addr localhost --m\u001b[0m\n",
      "\u001b[34master_port 41000\u001b[0m\n",
      "\u001b[34mSEQ_LEN=2048, HS=40\u001b[0m\n",
      "\u001b[34m96, FFN_HS=11008 TP=8 PP=1 N_LAYERS=32 N\u001b[0m\n",
      "\u001b[34m_AH=32 GBS=256 UBS=1 MIN_LR=1e-06\u001b[0m\n",
      "\u001b[34mINIT_METHOD_STD=,\u001b[0m\n",
      "\u001b[34mHIDDEN_DROPOUT=, LAYERNORM_EPSILON=\u001b[0m\n",
      "\u001b[34m1e-05, OPTIM_NAME=, OPTIM_LR=0.01\u001b[0m\n",
      "\u001b[34mOPTIM_WEIGHT_DECAY\u001b[0m\n",
      "\u001b[34m=0.1, OPTIM_SCHED_NAME=CosineAnneali\u001b[0m\n",
      "\u001b[34mng\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m**********\u001b[0m\n",
      "\u001b[34m*******************************\u001b[0m\n",
      "\u001b[34mSetting OM\u001b[0m\n",
      "\u001b[34mP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your\u001b[0m\n",
      "\u001b[34msystem being overloaded, please further tune the variab\u001b[0m\n",
      "\u001b[34mle for optimal performance in your appli\u001b[0m\n",
      "\u001b[34mcation as needed. \u001b[0m\n",
      "\u001b[34m*********************\u001b[0m\n",
      "\u001b[34m********************\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:35:59 optimizers:67] Could not import distributed_fused_adam optimizer from Apex\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:36:06 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:36:09 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f6f744784f0>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 15 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f7e1a82c340>\u001b[0m\n",
      "\u001b[34mUsing sep_token, but i\u001b[0m\n",
      "\u001b[34mt is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls\u001b[0m\n",
      "\u001b[34m_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, b\u001b[0m\n",
      "\u001b[34mut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 25 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fb36599ff10>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f862e3e2dd0>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 22 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fcc3916b220>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f997f658370>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 11 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f3007fdce80>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f5d51d43b50>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 20 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 2 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsi\u001b[0m\n",
      "\u001b[34mng pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fa3d51c7df0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f07372be7a0>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 12 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f6ec51dae00>\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-0\u001b[0m\n",
      "\u001b[34m3-18 21:36:13 nemo_log\u001b[0m\n",
      "\u001b[34mging:349] /usr/local/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job\u001b[0m\n",
      "\u001b[34mruntime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\u001b[0m\n",
      "\u001b[34mprecision plugi\u001b[0m\n",
      "\u001b[34mn:<pytorch_lightning.plugins.precision.tpu.TPUP\u001b[0m\n",
      "\u001b[34mrecisionPlugin object at 0x7f9b30fc84c0>\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03\u001b[0m\n",
      "\u001b[34m-18 21:36:13 megatron_gp\u001b[0m\n",
      "\u001b[34mt_pretraining:58] \n",
      "    \n",
      "    ************** Experiment configuration ***********\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 5 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch\u001b[0m\n",
      "\u001b[34m_calculator 10 None 25\u001b[0m\n",
      "\u001b[34m6 1 4\u001b[0m\n",
      "\u001b[34mYou are using th\u001b[0m\n",
      "\u001b[34me legacy behaviour of\u001b[0m\n",
      "\u001b[34mthe <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that to\u001b[0m\n",
      "\u001b[34mkens that come after special tokens will not be properly handled. We recommend\u001b[0m\n",
      "\u001b[34myou to read the related pull request available at https://github.com/huggingface/transformers/\u001b[0m\n",
      "\u001b[34mpull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f59cb82b2b0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f89aff53550>\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_gpt_pretraining:59] \n",
      "    name: megatron_llama\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 32\n",
      "      num_nodes: 1\n",
      "      accelerator: tpu\n",
      "      precision: 32\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: null\n",
      "      max_steps: 3\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 0.99\n",
      "      limit_val_batches: 1\n",
      "      limit_test_batches: 1\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      enable_model_summary: false\n",
      "    exp_manager:\n",
      "      create_tensorboard_logger: false\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: /tmp\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: step\n",
      "        save_top_k: -1\n",
      "        mode: max\n",
      "        save_last: true\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_llama--{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "        train_time_interval: 36000\n",
      "    model:\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 256\n",
      "      tensor_model_parallel_size: 8\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 32\n",
      "      hidden_size: 4096\n",
      "      ffn_hidden_size: 11008\n",
      "      num_attention_heads: 32\n",
      "      init_method_std: 0.021\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0\n",
      "      attention_dropout: 0\n",
      "      ffn_dropout: 0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 8\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      share_embeddings_and_output_weights: false\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1\n",
      "      activation: swiglu\n",
      "      transformer_block_type: pre_ln\n",
      "      has_bias: false\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: /opt/ml/additonals3data/tokenizer\n",
      "        model: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        delimiter: null\n",
      "        sentencepiece_legacy: false\n",
      "        use_fast: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: false\n",
      "      gradient_accumulation_fusion: false\n",
      "      bias_activation_fusion: false\n",
      "      bias_dropout_add_fusion: false\n",
      "      masked_softmax_fusion: false\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: /opt/ml/code/tmp/nemo_checkpoint/mp_rank_07/model_optim_rng.ckpt\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      log_parameter_norm: true\n",
      "      log_gradient_norm: true\n",
      "      activations_checkpoint_granularity: full\n",
      "      activations_checkpoint_method: uniform\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\u001b[0m\n",
      "\u001b[34msequence_parallel: true\n",
      "      wrap_with_zero: false\n",
      "      transformer_engine: false\n",
      "      fp8: false\n",
      "      fp8_e4m3: false\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1\n",
      "      fp8_amax_compute_algo: most_recent\n",
      "      use_emha: false\n",
      "      data:\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /opt/ml/code/tmp/tokenized_data_text_document\n",
      "        index_mapping_dir: null\n",
      "        data_impl: mmap\n",
      "        splits_string: 1000,0,0\n",
      "        seq_length: 2048\n",
      "        skip_warmup: true\n",
      "        num_workers: 1\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        validation_drop_last: true\n",
      "        no_seqlen_plus_one_input_tokens: false\n",
      "        pad_samples_to_global_batch_size: false\n",
      "      nsys_profile:\n",
      "        enabled: false\n",
      "        start_step: 10\n",
      "        end_step: 10\n",
      "        ranks:\n",
      "        - 0\n",
      "        gen_shape: false\n",
      "      optim:\n",
      "        name: adamw\n",
      "        lr: 0.01\n",
      "        weight_decay: 0.1\n",
      "        capturable: true\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 10\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-06\n",
      "      save_xser: true\n",
      "      load_xser: true\n",
      "    \u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f076d76e020>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7faf465e38\u001b[0m\n",
      "\u001b[34m20>\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.plug\u001b[0m\n",
      "\u001b[34mins.precision.tpu.TPU\u001b[0m\n",
      "\u001b[34mPrecisionPlugin objec\u001b[0m\n",
      "\u001b[34mt at 0x7f6dfbd6bcd\u001b[0m\n",
      "\u001b[34m0>\u001b[0m\n",
      "\u001b[34mprecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.plu\u001b[0m\n",
      "\u001b[34mgins.precision.tpu.TPUPrecisio\u001b[0m\n",
      "\u001b[34mnPlugin object at 0x7f45d656b2e0>\u001b[0m\n",
      "\u001b[34mprecision plugin:\u001b[0m\n",
      "\u001b[34m<pytorch_lightning.plug\u001b[0m\n",
      "\u001b[34mins.precision.tpu.TPUPrecis\u001b[0m\n",
      "\u001b[34mionPlugin o\u001b[0m\n",
      "\u001b[34mbject at 0x7ff03765a5f\u001b[0m\n",
      "\u001b[34m0>\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 14 None 2\u001b[0m\n",
      "\u001b[34m56 1 4\u001b[0m\n",
      "\u001b[34mYou are using t\u001b[0m\n",
      "\u001b[34mhe legacy behaviour of\u001b[0m\n",
      "\u001b[34mthe <class 'transformer\u001b[0m\n",
      "\u001b[34ms.models.\u001b[0m\n",
      "\u001b[34mllama.tokenizatio\u001b[0m\n",
      "\u001b[34mn_llama.LlamaTokeniz\u001b[0m\n",
      "\u001b[34mer'>. Th\u001b[0m\n",
      "\u001b[34mis means that to\u001b[0m\n",
      "\u001b[34mkens that come after\u001b[0m\n",
      "\u001b[34ms\u001b[0m\n",
      "\u001b[34mpecial\u001b[0m\n",
      "\u001b[34mtokens will not be properly handled. We re\u001b[0m\n",
      "\u001b[34mcommend you to read t\u001b[0m\n",
      "\u001b[34mhe relat\u001b[0m\n",
      "\u001b[34med pull re\u001b[0m\n",
      "\u001b[34mquest avai\u001b[0m\n",
      "\u001b[34mlable at htt\u001b[0m\n",
      "\u001b[34mps://github.com/hu\u001b[0m\n",
      "\u001b[34mgg\u001b[0m\n",
      "\u001b[34mingface/tr\u001b[0m\n",
      "\u001b[34mansformers/pull\u001b[0m\n",
      "\u001b[34m/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_t\u001b[0m\n",
      "\u001b[34moken, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, bu\u001b[0m\n",
      "\u001b[34mt it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34msetup_microba\u001b[0m\n",
      "\u001b[34mtch_calculator 16\u001b[0m\n",
      "\u001b[34mNone 256 1 4\u001b[0m\n",
      "\u001b[34mprecision plugi\u001b[0m\n",
      "\u001b[34mn:<pytorch_lightning.pl\u001b[0m\n",
      "\u001b[34mugins.precision.tpu.TPUPrecisionPlugin object at 0x7f203c3bbaf0>\u001b[0m\n",
      "\u001b[34mYou are using t\u001b[0m\n",
      "\u001b[34mhe legacy behaviour o\u001b[0m\n",
      "\u001b[34mf the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that toke\u001b[0m\n",
      "\u001b[34mns that come after special tokens will not be properly handled. We recommend you to re\u001b[0m\n",
      "\u001b[34mad the related pull request available at https://github.com/huggingface/transformers/pu\u001b[0m\n",
      "\u001b[34mll/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, b\u001b[0m\n",
      "\u001b[34mut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUs\u001b[0m\n",
      "\u001b[34ming pad_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch\u001b[0m\n",
      "\u001b[34m_calculator 3 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34msetup_microbatch_\u001b[0m\n",
      "\u001b[34mcalculator 29 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mYou are using the\u001b[0m\n",
      "\u001b[34mlegacy behaviour of the <\u001b[0m\n",
      "\u001b[34mclass 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that c\u001b[0m\n",
      "\u001b[34mome after special tokens will not be properly handled. We recommend you to read the\u001b[0m\n",
      "\u001b[34mrelated pull request available at https://github.com/huggingface/transformers/pull/\u001b[0m\n",
      "\u001b[34m24565\u001b[0m\n",
      "\u001b[34mYou are using the\u001b[0m\n",
      "\u001b[34mlegacy behaviour of t\u001b[0m\n",
      "\u001b[34mhe <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after special tokens will not be properly handled. We recommend you to read t\u001b[0m\n",
      "\u001b[34mhe related pull request available at https://github.com/huggingface/transformers\u001b[0m\n",
      "\u001b[34m/pull/24565\u001b[0m\n",
      "\u001b[34mprecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.p\u001b[0m\n",
      "\u001b[34mlugins.precision.tpu.TPUPrecisionPlugin object at 0x7f4503906560>\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 21 Non\u001b[0m\n",
      "\u001b[34me 256 1 4\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing cls_token\u001b[0m\n",
      "\u001b[34m, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 30 None\u001b[0m\n",
      "\u001b[34m256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly\u001b[0m\n",
      "\u001b[34mhandled. We recommend you to read\u001b[0m\n",
      "\u001b[34mthe related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy behaviour\u001b[0m\n",
      "\u001b[34mof the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly hand\u001b[0m\n",
      "\u001b[34mled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 31 None 256\u001b[0m\n",
      "\u001b[34m1 4\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f74da6b7d90>\u001b[0m\n",
      "\u001b[34mYou are using t\u001b[0m\n",
      "\u001b[34mhe legacy behaviour of\u001b[0m\n",
      "\u001b[34mthe <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not b\u001b[0m\n",
      "\u001b[34me properly handled. We recommend you to read the related pull request available at https://github.com/huggingfac\u001b[0m\n",
      "\u001b[34me/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fdb5aaebdc0>\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing mask_token, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing sep_token\u001b[0m\n",
      "\u001b[34m, but it is not set yet\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 8 None 256\u001b[0m\n",
      "\u001b[34m1 4\u001b[0m\n",
      "\u001b[34mUsing sep_to\u001b[0m\n",
      "\u001b[34mken, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing cls_t\u001b[0m\n",
      "\u001b[34moken, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34mUsing mask_\u001b[0m\n",
      "\u001b[34mtoken, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03\u001b[0m\n",
      "\u001b[34m-18 21:36:13 nlp_overri\u001b[0m\n",
      "\u001b[34mdes:418] NLPTrainer: Initializing trainer with parameters: {'self': <nemo.collections.nlp.parts.nlp_overrides.NLPTrainer object at 0x7fa5c16ab280>, 'logger': False, 'enabl\u001b[0m\n",
      "\u001b[34me_checkpointing': False, 'callbacks': None, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm':\u001b[0m\n",
      "\u001b[34mNone, 'num_nodes': 1, 'num_processes': None, 'devices': 32, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': No\u001b[0m\n",
      "\u001b[34mne, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': Fa\u001b[0m\n",
      "\u001b[34mlse, 'accumulate_grad_batches': 1, 'max_epochs': None, 'min_epochs': None, 'max_steps': 3, 'min_steps': None, 'max_time': None\u001b[0m\n",
      "\u001b[34m, 'limit_train_batches': None, 'limit_val_batches': 1, 'limit_test_batches': 1, 'limit_predict_batches': None, 'val_check_in\u001b[0m\n",
      "\u001b[34mterval': 0.99, 'log_every_n_steps': 1, 'accelerator': 'tpu', 'strategy': <nemo.collections.nlp.parts.nlp_overrides.NLPDDPStr\u001b[0m\n",
      "\u001b[34mategy object at 0x7fa5c169ff40>, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': False, 'num_sanity_val\u001b[0m\n",
      "\u001b[34m_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': None, 'reload\u001b[0m\n",
      "\u001b[34m_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_ba\u001b[0m\n",
      "\u001b[34mtch_size': False, 'plugins': [], 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainl\u001b[0m\n",
      "\u001b[34moader_mode': 'max_size_cycle', 'inference_mode': True}\u001b[0m\n",
      "\u001b[34msetup_microbatch\u001b[0m\n",
      "\u001b[34m_calculator 17 None 256\u001b[0m\n",
      "\u001b[34m1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recomm\u001b[0m\n",
      "\u001b[34mend you to read the related pull\u001b[0m\n",
      "\u001b[34mrequest available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision plu\u001b[0m\n",
      "\u001b[34mgin:<pytorch_lightn\u001b[0m\n",
      "\u001b[34ming.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fa5c16907f0>\u001b[0m\n",
      "\u001b[34mYou are usin\u001b[0m\n",
      "\u001b[34mg the legacy behav\u001b[0m\n",
      "\u001b[34miour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come\u001b[0m\n",
      "\u001b[34mafter special tokens will not be properly handled. We recommend you to read the related pull request available\u001b[0m\n",
      "\u001b[34mat https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mGPU available:\u001b[0m\n",
      "\u001b[34mTrue (cuda), used:\u001b[0m\n",
      "\u001b[34mFalse\u001b[0m\n",
      "\u001b[34mTPU available: True, using: 32 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18 21:36:13 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "      rank_zero_warn(\n",
      "    \u001b[0m\n",
      "\u001b[34m`Tra\u001b[0m\n",
      "\u001b[34miner(limit_val_batches=1)` was confi\u001b[0m\n",
      "\u001b[34mgured so 1 batch will be used.\u001b[0m\n",
      "\u001b[34m`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\u001b[0m\n",
      "\u001b[34mprecision plugin:\u001b[0m\n",
      "\u001b[34m<pytorch_lightning.plugi\u001b[0m\n",
      "\u001b[34mns.precision.tpu.TPUPrecisionPlugin object at 0x7f2929377ac0>\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 7 None 256\u001b[0m\n",
      "\u001b[34m1 4\u001b[0m\n",
      "\u001b[34mYou are using th\u001b[0m\n",
      "\u001b[34me legacy behaviour of t\u001b[0m\n",
      "\u001b[34mhe <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the\u001b[0m\n",
      "\u001b[34mrelated pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34m[NeMo E 2024-03\u001b[0m\n",
      "\u001b[34m-18 21:36:13 exp_manage\u001b[0m\n",
      "\u001b[34mr:465] You are running multi-gpu without ddp.Please note that this is not tested in NeMo and could result in errors.\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03\u001b[0m\n",
      "\u001b[34m-18 21:36:13 exp_manage\u001b[0m\n",
      "\u001b[34mr:350] Experiments will be logged at /tmp/megatron_llama/2024-03-18_21-35-41\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-\u001b[0m\n",
      "\u001b[34m03-18 21:36:13 nemo_l\u001b[0m\n",
      "\u001b[34mogging:349] /usr/local/lib/python3.10/site-packages/nemo/utils/exp_manager.py:1035: UserWarning: Detected custom epoch loop. Skipping no validation on restart support.\n",
      "      warnings.warn(\"Detect\u001b[0m\n",
      "\u001b[34med custom epoch loop. Skipping no validation on restart support.\", UserWarning)\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 19 None 25\u001b[0m\n",
      "\u001b[34m6 1 4\u001b[0m\n",
      "\u001b[34mYou are using th\u001b[0m\n",
      "\u001b[34me legacy behaviour of th\u001b[0m\n",
      "\u001b[34me <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request av\u001b[0m\n",
      "\u001b[34mailable at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03\u001b[0m\n",
      "\u001b[34m-18 21:36:13 megatron\u001b[0m\n",
      "\u001b[34m_gpt_pretraining:103] Resuming training from checkpoint: /opt/ml/code/tmp/nemo_checkpoint/mp_rank_07/model_optim_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-0\u001b[0m\n",
      "\u001b[34m3-18 21:36:1\u001b[0m\n",
      "\u001b[34m3 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Traine\u001b[0m\n",
      "\u001b[34mr(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\u001b[0m\n",
      "\u001b[34mrank_zero_deprecation(\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 27 None\u001b[0m\n",
      "\u001b[34m256 1 4\u001b[0m\n",
      "\u001b[34mYou are using t\u001b[0m\n",
      "\u001b[34mhe legacy behaviour of\u001b[0m\n",
      "\u001b[34mthe <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch\u001b[0m\n",
      "\u001b[34m_calculator 18 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy behaviour\u001b[0m\n",
      "\u001b[34mof the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you t\u001b[0m\n",
      "\u001b[34mo read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 23 None 2\u001b[0m\n",
      "\u001b[34m56 1 4\u001b[0m\n",
      "\u001b[34mYou are using t\u001b[0m\n",
      "\u001b[34mhe legacy behaviour\u001b[0m\n",
      "\u001b[34mof the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the r\u001b[0m\n",
      "\u001b[34melated pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set ye\u001b[0m\n",
      "\u001b[34mt.\u001b[0m\n",
      "\u001b[34mUsing cls_to\u001b[0m\n",
      "\u001b[34mken, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing pad_t\u001b[0m\n",
      "\u001b[34moken, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set yet\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugi\u001b[0m\n",
      "\u001b[34mn:<pytorch_lightning.p\u001b[0m\n",
      "\u001b[34mlugins.precision.tpu.TPUPrecisionPlugin object at 0x7fd841cd7df0>\u001b[0m\n",
      "\u001b[34mprecision plugin:\u001b[0m\n",
      "\u001b[34m<pytorch_lightning.plugi\u001b[0m\n",
      "\u001b[34mns.precision.tpu.TPUPrecisionPlugin object at 0x7fe2a920ba00>\u001b[0m\n",
      "\u001b[34mUsing sep_token\u001b[0m\n",
      "\u001b[34m, but it is not set yet\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugi\u001b[0m\n",
      "\u001b[34mn:<pytorch_lightning.pl\u001b[0m\n",
      "\u001b[34mugins.precision.tpu.TPUPrecisionPlugin object at 0x7f3e8a379b10>\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set y\u001b[0m\n",
      "\u001b[34met.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask\u001b[0m\n",
      "\u001b[34m_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.plug\u001b[0m\n",
      "\u001b[34mins.precision.tpu.TPUPrecisionPlugin object at 0x7eff6ac28160>\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-1\u001b[0m\n",
      "\u001b[34m8 21:36:13 megatron_init:2\u001b[0m\n",
      "\u001b[34m55] Rank 0 has data parallel group: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 m\u001b[0m\n",
      "\u001b[34megatron_init:258] All data parallel group ranks: [[0, 8, 16, 24], [1, 9, 17, 25],\u001b[0m\n",
      "\u001b[34m[2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13\u001b[0m\n",
      "\u001b[34mmegatron_init:259] Ranks 0 has data parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_\u001b[0m\n",
      "\u001b[34minit:267] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:268] Al\u001b[0m\n",
      "\u001b[34ml model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [\u001b[0m\n",
      "\u001b[34m24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:278] Rank 0 has tensor model parallel group: [0, 1, 2,\u001b[0m\n",
      "\u001b[34m3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:282] All tensor model pa\u001b[0m\n",
      "\u001b[34mrallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24,\u001b[0m\n",
      "\u001b[34m25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:283] Rank 0 has tensor model parallel rank: 0\u001b[0m\n",
      "\u001b[34m[Ne\u001b[0m\n",
      "\u001b[34mMo I 2024-03-18 21:36:13 megatron_init:297] Rank 0 has pipeline model parallel grou\u001b[0m\n",
      "\u001b[34mp: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:309] Rank 0 has embedding group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:315] All pipeline model parallel grou\u001b[0m\n",
      "\u001b[34mp ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]\u001b[0m\n",
      "\u001b[34m, [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:316]\u001b[0m\n",
      "\u001b[34mRank 0 has pipeline model parallel rank 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:317] All\u001b[0m\n",
      "\u001b[34membedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25]\u001b[0m\n",
      "\u001b[34m, [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_init:318] Rank 0 has embedding rank: 0\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 0 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34m24-03-18 21:36:13 - PID:291 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of\u001b[0m\n",
      "\u001b[34mmicro-batches to constant 64\u001b[0m\n",
      "\u001b[34m[NeMo I 2024\u001b[0m\n",
      "\u001b[34m-03-18 21:36:13 tok\u001b[0m\n",
      "\u001b[34menizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/addit\u001b[0m\n",
      "\u001b[34monals3data/tokenizer\u001b[0m\n",
      "\u001b[34mprecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.plu\u001b[0m\n",
      "\u001b[34mgins.precision.tpu.TPUPrecisionPlugin object at 0x7f7e4876d030>\u001b[0m\n",
      "\u001b[34mprecision plugi\u001b[0m\n",
      "\u001b[34mn:<pytorch_lightning.pl\u001b[0m\n",
      "\u001b[34mugins.precision.tpu.TPUPrecisionPlugin object at 0x7f11f004ba00>\u001b[0m\n",
      "\u001b[34mYou are using the\u001b[0m\n",
      "\u001b[34mlegacy behaviour of the\u001b[0m\n",
      "\u001b[34m<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens th\u001b[0m\n",
      "\u001b[34mat come after special tokens will not be properly handled. We recommend you to read the related pull request\u001b[0m\n",
      "\u001b[34mavailable at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_\u001b[0m\n",
      "\u001b[34mcalculator 26 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mYou are using th\u001b[0m\n",
      "\u001b[34me legacy behaviour of the\u001b[0m\n",
      "\u001b[34m<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after special tokens will not be properly handled. We recommend you to read the related pull requ\u001b[0m\n",
      "\u001b[34mest available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:\u001b[0m\n",
      "\u001b[34m<pytorch_lightning.plugi\u001b[0m\n",
      "\u001b[34mns.precision.tpu.TPUPrecisionPlugin object at 0x7f60aca7fd00>\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not s\u001b[0m\n",
      "\u001b[34met yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:13 megatron_base_model:275] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 4 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 6 None 256 1 4\u001b[0m\n",
      "\u001b[34mUsing cls_tok\u001b[0m\n",
      "\u001b[34men, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy behaviour\u001b[0m\n",
      "\u001b[34mof the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that token\u001b[0m\n",
      "\u001b[34ms that come after special tokens will not be properly handled. We recommend you to read the rela\u001b[0m\n",
      "\u001b[34mted pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are u\u001b[0m\n",
      "\u001b[34msing the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokeniz\u001b[0m\n",
      "\u001b[34mer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request availab\u001b[0m\n",
      "\u001b[34mle at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34m[NeMo W 2024-03-18\u001b[0m\n",
      "\u001b[34m21:36:13 nemo_logging:3\u001b[0m\n",
      "\u001b[34m49] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1908: LightningDeprecationWarning: `tr\u001b[0m\n",
      "\u001b[34mainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v2.0. Specify the fit checkp\u001b[0m\n",
      "\u001b[34moint path with `trainer.fit(ckpt_path=)` instead.\n",
      "      rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 24 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_\u001b[0m\n",
      "\u001b[34mtoken, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34mUsing pad_t\u001b[0m\n",
      "\u001b[34moken, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34mUsing mask\u001b[0m\n",
      "\u001b[34m_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34msetup_microba\u001b[0m\n",
      "\u001b[34mtch_calculator 1 Non\u001b[0m\n",
      "\u001b[34me 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using t\u001b[0m\n",
      "\u001b[34mhe legacy behaviour of\u001b[0m\n",
      "\u001b[34mthe <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means\u001b[0m\n",
      "\u001b[34mthat tokens that come after special tokens will not be properly handled. We recommend you to read the\u001b[0m\n",
      "\u001b[34mrelated pull request available at https://github.com/huggingface/transform\u001b[0m\n",
      "\u001b[34mers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_cal\u001b[0m\n",
      "\u001b[34mculator 13 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the leg\u001b[0m\n",
      "\u001b[34macy behaviour of the <\u001b[0m\n",
      "\u001b[34mclass 'transformers.models.llama.tokenizatio\u001b[0m\n",
      "\u001b[34mn_llama.LlamaTokenizer'\u001b[0m\n",
      "\u001b[34m>. This means that tokens that come after special tokens will\u001b[0m\n",
      "\u001b[34mnot be properly handled. We recomm\u001b[0m\n",
      "\u001b[34mend you to read the related pull request a\u001b[0m\n",
      "\u001b[34mvailable at https://github.com/hugging\u001b[0m\n",
      "\u001b[34mface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_to\u001b[0m\n",
      "\u001b[34mken, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set y\u001b[0m\n",
      "\u001b[34met.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 9 Non\u001b[0m\n",
      "\u001b[34me 256 1 4\u001b[0m\n",
      "\u001b[34mYou are\u001b[0m\n",
      "\u001b[34musing the legac\u001b[0m\n",
      "\u001b[34my behaviour of the <class 'transforme\u001b[0m\n",
      "\u001b[34mrs.models.llama.tokenization_llama.Ll\u001b[0m\n",
      "\u001b[34mamaTokenizer'>. This means that tokens that come after special tokens wi\u001b[0m\n",
      "\u001b[34mll not be properly handled. We recommend you to read\u001b[0m\n",
      "\u001b[34mthe related pull request available at https://github.com/hugging\u001b[0m\n",
      "\u001b[34mface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, b\u001b[0m\n",
      "\u001b[34mut it is not set ye\u001b[0m\n",
      "\u001b[34mt.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34mmask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_ca\u001b[0m\n",
      "\u001b[34mlculator 28 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are u\u001b[0m\n",
      "\u001b[34msing the leg\u001b[0m\n",
      "\u001b[34macy behaviour of the <class 't\u001b[0m\n",
      "\u001b[34mransformers.models.llama.tokenizati\u001b[0m\n",
      "\u001b[34mon_llama.LlamaTokenizer'>. Th\u001b[0m\n",
      "\u001b[34mis means that tokens that come after\u001b[0m\n",
      "\u001b[34mspecial tokens will not be proper\u001b[0m\n",
      "\u001b[34mly handled. We recommend you to read the related pull reque\u001b[0m\n",
      "\u001b[34mst available at https://github.co\u001b[0m\n",
      "\u001b[34mm/huggingface/transformers/pull/\u001b[0m\n",
      "\u001b[34m24565\u001b[0m\n",
      "\u001b[34mUsing s\u001b[0m\n",
      "\u001b[34mep_token, b\u001b[0m\n",
      "\u001b[34mut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsi\u001b[0m\n",
      "\u001b[34mng cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsin\u001b[0m\n",
      "\u001b[34mg pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:1 to store for ran\u001b[0m\n",
      "\u001b[34mk: 31\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 23\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank:\u001b[0m\n",
      "\u001b[34m24\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:1 t\u001b[0m\n",
      "\u001b[34mo store for rank: 1\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 t\u001b[0m\n",
      "\u001b[34mo store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 t\u001b[0m\n",
      "\u001b[34mo store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:1 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 6\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank:\u001b[0m\n",
      "\u001b[34m18\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1\u001b[0m\n",
      "\u001b[34mto store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:1 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank: 21\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank:\u001b[0m\n",
      "\u001b[34m29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:1 to store for rank: 26\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 to\u001b[0m\n",
      "\u001b[34mstore for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:1 to store for r\u001b[0m\n",
      "\u001b[34mank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:1 t\u001b[0m\n",
      "\u001b[34mo store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 13\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:1 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank:\u001b[0m\n",
      "\u001b[34m29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed\u001b[0m\n",
      "\u001b[34mstore-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:1 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier\u001b[0m\n",
      "\u001b[34m_key:2 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:2 to store for\u001b[0m\n",
      "\u001b[34mrank: 10\u001b[0m\n",
      "\u001b[34mRank 2\u001b[0m\n",
      "\u001b[34m5: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:1 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_barrier_key:1 with 32 n\u001b[0m\n",
      "\u001b[34modes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:2 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for\u001b[0m\n",
      "\u001b[34mrank: 25\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:1 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrier_key:1 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 31\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 18: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to\u001b[0m\n",
      "\u001b[34mstore for rank: 17\u001b[0m\n",
      "\u001b[34mRank 11:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_barrier_key:1\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank\u001b[0m\n",
      "\u001b[34m: 6\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for\u001b[0m\n",
      "\u001b[34mrank: 18\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 20: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:2 to store for r\u001b[0m\n",
      "\u001b[34mank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store\u001b[0m\n",
      "\u001b[34mfor rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m2 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:2 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 26: C\u001b[0m\n",
      "\u001b[34mompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to stor\u001b[0m\n",
      "\u001b[34me for rank: 26\u001b[0m\n",
      "\u001b[34mRank 14: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_based_barrier_key:1 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for ran\u001b[0m\n",
      "\u001b[34mk: 21\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:1 with 32 no\u001b[0m\n",
      "\u001b[34mdes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:2 to store for rank:\u001b[0m\n",
      "\u001b[34m24\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 3: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for ran\u001b[0m\n",
      "\u001b[34mk: 3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34mR\u001b[0m\n",
      "\u001b[34mank 30: Completed store-based barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:2 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 9: C\u001b[0m\n",
      "\u001b[34mompleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrier_key:1 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 23: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:2 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m1 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:2 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 0\u001b[0m\n",
      "\u001b[34mRank 27: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Complete\u001b[0m\n",
      "\u001b[34md store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 27\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:2 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 8: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:2 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 8\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key:2 with 32 n\u001b[0m\n",
      "\u001b[34modes.\u001b[0m\n",
      "\u001b[34mRank 29: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:3 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 15: C\u001b[0m\n",
      "\u001b[34mompleted store-based barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:3 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 28: Comp\u001b[0m\n",
      "\u001b[34mleted store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:2 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 25: Comp\u001b[0m\n",
      "\u001b[34mleted store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key:2 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store\u001b[0m\n",
      "\u001b[34mfor rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m3 to store for rank: 25\u001b[0m\n",
      "\u001b[34mRank 31:\u001b[0m\n",
      "\u001b[34mCompleted store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:2 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:2 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 31\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 6: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 t\u001b[0m\n",
      "\u001b[34mo store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m22: Completed store-based barrier for key:store_based_barrier_key:2 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:2 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mto store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:3 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:3 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 26\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:3 to store for rank:\u001b[0m\n",
      "\u001b[34m21\u001b[0m\n",
      "\u001b[34mRank 14: Completed\u001b[0m\n",
      "\u001b[34mstore-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:3 to store for rank: 24\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 3: Completed store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_barrier_key:2 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to\u001b[0m\n",
      "\u001b[34mstore for rank: 3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:2 with 32 no\u001b[0m\n",
      "\u001b[34mdes.\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_key:2 with 32 n\u001b[0m\n",
      "\u001b[34modes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:3 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 9:\u001b[0m\n",
      "\u001b[34mCompleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:3 to store for\u001b[0m\n",
      "\u001b[34mrank: 9\u001b[0m\n",
      "\u001b[34mRank 23:\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\u001b[34md store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mto store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:3 to store for rank\u001b[0m\n",
      "\u001b[34m: 0\u001b[0m\n",
      "\u001b[34mRank 27:\u001b[0m\n",
      "\u001b[34mCompleted s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:2 with 32 n\u001b[0m\n",
      "\u001b[34modes.\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:2 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m3 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:3 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:4 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 8: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Completed store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store\u001b[0m\n",
      "\u001b[34mfor rank: 8\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m3: Compl\u001b[0m\n",
      "\u001b[34meted store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:4 to st\u001b[0m\n",
      "\u001b[34more for rank: 29\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to st\u001b[0m\n",
      "\u001b[34more for rank: 15\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:4 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 28: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:3 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for ran\u001b[0m\n",
      "\u001b[34mk: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:4 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 31:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:3 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for r\u001b[0m\n",
      "\u001b[34mank: 31\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store\u001b[0m\n",
      "\u001b[34mfor rank: 1\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:3\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 17: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 6: C\u001b[0m\n",
      "\u001b[34mompleted store-based barrier for key:store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_based_barrier_key:3 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to stor\u001b[0m\n",
      "\u001b[34me for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:4 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 2\u001b[0m\n",
      "\u001b[34m2: Completed store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mto store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:4 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to\u001b[0m\n",
      "\u001b[34mstore for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:4 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrier_key:3 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank:\u001b[0m\n",
      "\u001b[34m5\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:4 to store for rank:\u001b[0m\n",
      "\u001b[34m26\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:3 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:3 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:4 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 24: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 3: Completed store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:3 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_ke\u001b[0m\n",
      "\u001b[34my:4 to sto\u001b[0m\n",
      "\u001b[34mre for rank:\u001b[0m\n",
      "\u001b[34m24\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mR\u001b[0m\n",
      "\u001b[34mank 12: Completed store-based barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:4 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m9: Completed store-based barrier for key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 23:\u001b[0m\n",
      "\u001b[34mCompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:4 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 27:\u001b[0m\n",
      "\u001b[34mCompleted sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:4 to\u001b[0m\n",
      "\u001b[34mstore for rank: 27\u001b[0m\n",
      "\u001b[34mRank 27:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: C\u001b[0m\n",
      "\u001b[34mompleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 t\u001b[0m\n",
      "\u001b[34mo store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:5 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 8: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_barrier_key:4 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:4 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m5: Completed store-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mto store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m5 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrier_key:4 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:4 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store\u001b[0m\n",
      "\u001b[34mfor rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:5 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 25:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for\u001b[0m\n",
      "\u001b[34mrank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:5 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 31\u001b[0m\n",
      "\u001b[34m: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:5 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 1: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank\u001b[0m\n",
      "\u001b[34m: 1\u001b[0m\n",
      "\u001b[34mRank 17: Comple\u001b[0m\n",
      "\u001b[34mted store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 18: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m6: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:s\u001b[0m\n",
      "\u001b[34mtore_based\u001b[0m\n",
      "\u001b[34m_barrier_key:4 with 32 node\u001b[0m\n",
      "\u001b[34ms.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 to store for rank: 22\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 to store for rank: 18\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store\u001b[0m\n",
      "\u001b[34mfor rank: 4\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:5 to store f\u001b[0m\n",
      "\u001b[34mor rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to\u001b[0m\n",
      "\u001b[34mstore for rank: 20\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:5 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 5\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:5 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m4: Completed store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:4 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:5 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_barrier_key:4 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:4 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:4 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 9: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:4 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key: store_based_barrier_key:5 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:5 to st\u001b[0m\n",
      "\u001b[34more for rank: 0\u001b[0m\n",
      "\u001b[34mRank 0: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 to store for rank\u001b[0m\n",
      "\u001b[34m: 0\u001b[0m\n",
      "\u001b[34mRank 27: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:6\u001b[0m\n",
      "\u001b[34mto store for rank: 27\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 8: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 to store for rank: 29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 10: Comple\u001b[0m\n",
      "\u001b[34mted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m6: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:5 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:6 to store for rank:\u001b[0m\n",
      "\u001b[34m16\u001b[0m\n",
      "\u001b[34mRank 25: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2\u001b[0m\n",
      "\u001b[34m8: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:6 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:6 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 31:\u001b[0m\n",
      "\u001b[34mCompleted st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:5 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6\u001b[0m\n",
      "\u001b[34mto store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 6: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:6 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to\u001b[0m\n",
      "\u001b[34mstore for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:6 to store for rank: 18\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 17: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:6 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:6 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to st\u001b[0m\n",
      "\u001b[34more for rank: 5\u001b[0m\n",
      "\u001b[34mRank 26: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:6 to store\u001b[0m\n",
      "\u001b[34mfor rank: 26\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:store_based_barrier_key:5 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:6 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m7: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m12: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:\u001b[0m\n",
      "\u001b[34m6 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 2: Completed store-based barrier for key:store_based_barrier_key:5 with 32 node\u001b[0m\n",
      "\u001b[34ms.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m9: Completed store-based barrier for key:store_based_barrier_key:5 with 32 no\u001b[0m\n",
      "\u001b[34mdes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank:\u001b[0m\n",
      "\u001b[34m9\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:6 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 9:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:7 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 9\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m7 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:7 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 8: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2\u001b[0m\n",
      "\u001b[34m9: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:7 to store for ran\u001b[0m\n",
      "\u001b[34mk: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 29\u001b[0m\n",
      "\u001b[34mRank 13: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_barrier_key:6 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:6 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 10: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:6 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:7 to store\u001b[0m\n",
      "\u001b[34mfor rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 25: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to\u001b[0m\n",
      "\u001b[34mstore for rank: 25\u001b[0m\n",
      "\u001b[34mRank 31:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:\u001b[0m\n",
      "\u001b[34m7 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 1: C\u001b[0m\n",
      "\u001b[34mompleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:6 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 6: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_barrier_key:6 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 17: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_based_barrier_key:6 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to\u001b[0m\n",
      "\u001b[34mstore for rank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:7 to store for rank: 18\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:7 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m5: Completed store-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:7 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store\u001b[0m\n",
      "\u001b[34mfor rank: 5\u001b[0m\n",
      "\u001b[34mRank 26: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_key:6\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:7 to store for ran\u001b[0m\n",
      "\u001b[34mk: 26\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:7 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 21: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:7 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3:\u001b[0m\n",
      "\u001b[34mCompleted\u001b[0m\n",
      "\u001b[34mstore-based ba\u001b[0m\n",
      "\u001b[34mrrier fo\u001b[0m\n",
      "\u001b[34mr key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:7 to store for r\u001b[0m\n",
      "\u001b[34mank: 3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mRank 7:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 30: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:7 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:8 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 23: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to\u001b[0m\n",
      "\u001b[34mstore for rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for ran\u001b[0m\n",
      "\u001b[34mk: 0\u001b[0m\n",
      "\u001b[34mRank 27: Co\u001b[0m\n",
      "\u001b[34mmpleted store-base\u001b[0m\n",
      "\u001b[34md barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:7 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mRank 19: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_ke\u001b[0m\n",
      "\u001b[34my:8 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m9\u001b[0m\n",
      "\u001b[34mRank 8\u001b[0m\n",
      "\u001b[34m: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 29: Comp\u001b[0m\n",
      "\u001b[34mleted store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed stor\u001b[0m\n",
      "\u001b[34me-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m9\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 32 n\u001b[0m\n",
      "\u001b[34modes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_ke\u001b[0m\n",
      "\u001b[34my:8 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:8 to store for\u001b[0m\n",
      "\u001b[34mrank: 13\u001b[0m\n",
      "\u001b[34mRank 10\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:8 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:8 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 28: Complet\u001b[0m\n",
      "\u001b[34med store-based barr\u001b[0m\n",
      "\u001b[34mier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:8 to\u001b[0m\n",
      "\u001b[34mstore for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank\u001b[0m\n",
      "\u001b[34m: 25\u001b[0m\n",
      "\u001b[34mRank 3\u001b[0m\n",
      "\u001b[34m1: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:7 with 32 n\u001b[0m\n",
      "\u001b[34modes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store f\u001b[0m\n",
      "\u001b[34mor rank: 31\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:8 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 1\u001b[0m\n",
      "\u001b[34mRank 22: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:7 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for ran\u001b[0m\n",
      "\u001b[34mk: 22\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m6: Completed store-based barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8\u001b[0m\n",
      "\u001b[34mto store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 18\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:8 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:8 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:7 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:8 to store for rank: 26\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 21:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_barrier_key:7 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Com\u001b[0m\n",
      "\u001b[34mpleted store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:8 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 7: Co\u001b[0m\n",
      "\u001b[34mmpleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m12: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:8 to store\u001b[0m\n",
      "\u001b[34mfor rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 2: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store\u001b[0m\n",
      "\u001b[34mfor rank: 2\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 30: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:9 to store for r\u001b[0m\n",
      "\u001b[34mank: 2\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store\u001b[0m\n",
      "\u001b[34mfor rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:9 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:9 to stor\u001b[0m\n",
      "\u001b[34me for rank: 0\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:8 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:9 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 19\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:9 to store f\u001b[0m\n",
      "\u001b[34mor rank: 8\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 29\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m5: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:8 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m9 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 10:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:8 with 32 node\u001b[0m\n",
      "\u001b[34ms.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:9 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:9 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 28:\u001b[0m\n",
      "\u001b[34mCompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:9 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:9 to store for rank: 25\u001b[0m\n",
      "\u001b[34mRank 31: Compl\u001b[0m\n",
      "\u001b[34meted store-based barr\u001b[0m\n",
      "\u001b[34mier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:9 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 1: Comp\u001b[0m\n",
      "\u001b[34mleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for ran\u001b[0m\n",
      "\u001b[34mk: 1\u001b[0m\n",
      "\u001b[34mRank 22: Completed\u001b[0m\n",
      "\u001b[34mstore-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:8 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 11: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:8 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank:\u001b[0m\n",
      "\u001b[34m22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:8 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:9 to store for\u001b[0m\n",
      "\u001b[34mrank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:9 to store for rank: 18\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:9 to store for\u001b[0m\n",
      "\u001b[34mrank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for ran\u001b[0m\n",
      "\u001b[34mk: 6\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_barrier_key:8 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for r\u001b[0m\n",
      "\u001b[34mank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 26\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:9 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 24: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 3: C\u001b[0m\n",
      "\u001b[34mompleted store-based barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:9 to store for rank: 24\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m12: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 7: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 7 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 2: Co\u001b[0m\n",
      "\u001b[34mmpleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34ms\u001b[0m\n",
      "\u001b[34metup_microbatch_calculator 12 None\u001b[0m\n",
      "\u001b[34m256 1 4\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calculato\u001b[0m\n",
      "\u001b[34mr 2 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 30: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 23 No\u001b[0m\n",
      "\u001b[34mne 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 30 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 9 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 0: Complete\u001b[0m\n",
      "\u001b[34md store-based barr\u001b[0m\n",
      "\u001b[34mier for key:store_based_barrier_key:9 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 0\u001b[0m\n",
      "\u001b[34mNone 256 1 4\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m9: Completed store-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 27 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 19 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microb\u001b[0m\n",
      "\u001b[34match_calculator 8 N\u001b[0m\n",
      "\u001b[34mone 256 1 4\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-base\u001b[0m\n",
      "\u001b[34md barrier for key:store_based_barrier_key:9 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:9 with 32 node\u001b[0m\n",
      "\u001b[34ms.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:9 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_\u001b[0m\n",
      "\u001b[34mcalculator 13 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calculator 29 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 10: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_based_barrier_key:9 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 15 None 256 1 4\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 16: Completed store-based barrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 1\u001b[0m\n",
      "\u001b[34m0 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 16 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 25 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 28 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 1: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 31: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 1 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 31 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfo\u001b[0m\n",
      "\u001b[34mr key:store_ba\u001b[0m\n",
      "\u001b[34msed_\u001b[0m\n",
      "\u001b[34mbarrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 4: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_m\u001b[0m\n",
      "\u001b[34microbatch_calculator 22 None 256 1 4\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 5: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup\u001b[0m\n",
      "\u001b[34m_microbatch_calculator 11 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 4 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 18\u001b[0m\n",
      "\u001b[34m: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:9\u001b[0m\n",
      "\u001b[34mwith 32 node\u001b[0m\n",
      "\u001b[34ms.\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key:9\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 26: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch\u001b[0m\n",
      "\u001b[34m_calculator 6 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 18 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 26 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 20 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 5 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 24 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 3 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 14 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calcul\u001b[0m\n",
      "\u001b[34mator 21 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculato\u001b[0m\n",
      "\u001b[34mr 17 None 256 1 4\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_gpt_model:940] Started profiling server for dp_rank=0, tp_rank=0, pp_rank=0, vp_rank=None on port:9000\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:255] Rank 0 has data parallel group: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:258] All data parallel group ranks: [[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:259] Ranks 0 has data parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:267] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:268] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:278] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:282] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:283] Rank 0 has tensor model parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:297] Rank 0 has pipeline model parallel group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:309] Rank 0 has embedding group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:315] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16],\u001b[0m\n",
      "\u001b[34m[17], [18], [19], [20], [\u001b[0m\n",
      "\u001b[34m21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatro\u001b[0m\n",
      "\u001b[34mn_init:316] Rank 0 has pipeline model parallel ran\u001b[0m\n",
      "\u001b[34mk 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36\u001b[0m\n",
      "\u001b[34m:20 megatron_init\u001b[0m\n",
      "\u001b[34m:317] All embedding group ranks: [[0], [1], [2],\u001b[0m\n",
      "\u001b[34m[3], [4], [5], [6], [7], [8], [9], [\u001b[0m\n",
      "\u001b[34m10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [\u001b[0m\n",
      "\u001b[34m20], [21],\u001b[0m\n",
      "\u001b[34m[22], [23], [24], [25], [26], [27], [2\u001b[0m\n",
      "\u001b[34m8], [29\u001b[0m\n",
      "\u001b[34m], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2024-03-18 21:36:20 megatron_init:318] Rank 0 has embedding rank: 0\u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:tdrv_init_mla_phase1                    Could not open the nd1\u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized! \u001b[0m\n",
      "\u001b[34m2024-Mar-18 21:36:20.0344   680:8906  ERROR  TDRV:notification_destroy                    Notifications not initialized!\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeli\u001b[0m\n",
      "\u001b[34mne) model parallel rank (2, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel\u001b[0m\n",
      "\u001b[34mrank (3, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline\u001b[0m\n",
      "\u001b[34m) model parallel rank (5, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (7, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (4, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (0, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (1, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (6, 0): 842534912\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 291 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 292 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 293 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 294 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 295 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 296 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 297 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 298 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 300 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 301 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 302 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:tor\u001b[0m\n",
      "\u001b[34mch.distributed.elastic.multiprocessing.api:Sending process 303 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 304 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 305 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 306 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 307 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distri\u001b[0m\n",
      "\u001b[34mbuted.elastic.multiprocessing.api:Sending process 308 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 309 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 310 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 311 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 312 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 313 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 314 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 315 cl\u001b[0m\n",
      "\u001b[34mosing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31\u001b[0m\n",
      "\u001b[34m6\u001b[0m\n",
      "\u001b[34mclosing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 317 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 318 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 319 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 320 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 321 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 322 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/usr/local/bin/torch\u001b[0m\n",
      "\u001b[34mrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  Fil\u001b[0m\n",
      "\u001b[34me \"/usr/local/lib/python3.10/site-\u001b[0m\n",
      "\u001b[34mpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line\u001b[0m\n",
      "\u001b[34m346, in wrapper\n",
      "    return f(*args,\u001b[0m\n",
      "\u001b[34m**kwargs)\n",
      "  File \"/usr/local/lib/python\u001b[0m\n",
      "\u001b[34m3.10/site-packages/torch/distributed/run.py\",\u001b[0m\n",
      "\u001b[34mline 762, in main\u001b[0m\n",
      "\u001b[34mrun(args)\n",
      "  File \"/usr/local/lib/pytho\u001b[0m\n",
      "\u001b[34mn3.10/site-pac\u001b[0m\n",
      "\u001b[34mkages/torch/distributed/run.py\", line 753, in ru\u001b[0m\n",
      "\u001b[34mn\u001b[0m\n",
      "\u001b[34melastic_launch(\n",
      "  Fi\u001b[0m\n",
      "\u001b[34mle \"/usr/local/lib/python3.10/site-packages/torch/di\u001b[0m\n",
      "\u001b[34mstributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entry\u001b[0m\n",
      "\u001b[34mpoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/\u001b[0m\n",
      "\u001b[34msite-packages/torch/distributed/launcher/api.py\", line 237, in launch_ag\u001b[0m\n",
      "\u001b[34ment\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/tor\u001b[0m\n",
      "\u001b[34mch/distributed/elastic/metrics/api.py\", line\u001b[0m\n",
      "\u001b[34m129, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/pyt\u001b[0m\n",
      "\u001b[34mhon3.10/site-packag\u001b[0m\n",
      "\u001b[34mes/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n",
      "    result\u001b[0m\n",
      "\u001b[34m= self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/si\u001b[0m\n",
      "\u001b[34mte-packages/torch/distributed/elastic/agent/server/api.py\",\u001b[0m\n",
      "\u001b[34mline 850, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/lo\u001b[0m\n",
      "\u001b[34mcal/lib/python3.10/site-packages/torch/distributed/elast\u001b[0m\n",
      "\u001b[34mic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
      "    raise SignalException(f\"\u001b[0m\n",
      "\u001b[34mProcess {os.getpid()} got signal: {sigval}\", sigval=\u001b[0m\n",
      "\u001b[34msigval)\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.api.SignalException: Process 2\u001b[0m\n",
      "\u001b[34m80 got signal: 15\u001b[0m\n",
      "\u001b[34m2024-03-18 21:36:28.000827:  274  ERROR ||NEURON_PARALLEL_COMPILE||: There was an error in the training script.\u001b[0m\n",
      "\u001b[34mERROR:root:Subprocess script failed with return code: 1\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/sagemaker_jumpstart_script_utilities/subprocess.py\", line 9, in run_with_error_handling\u001b[0m\n",
      "\u001b[34msubprocess.run(command, shell=shell, check=True)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 526, in run\u001b[0m\n",
      "\u001b[34mraise CalledProcessError(retcode, process.args,\u001b[0m\n",
      "\u001b[34msubprocess.CalledProcessError: Command '['neuron_parallel_compile', './fine_tuning_trn1.sh']' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/transfer_learning.py\", line 264, in <module>\u001b[0m\n",
      "\u001b[34mrun_with_args(args)\n",
      "  File \"/opt/ml/code/transfer_learning.py\", line 235, in run_with_args\u001b[0m\n",
      "\u001b[34msubprocess.run_with_error_handling([\"neuron_parallel_compile\", f\"./{fs_constants.FINE_TUNING_SCRIPT_BASH_FILE}\"])\n",
      "  File \"/usr/local/lib/python3.10/site-packages/sagemaker_jumpstart_script_utilities/subprocess.py\", line 12, in run_with_error_handling\n",
      "    raise RuntimeError(e)\u001b[0m\n",
      "\u001b[34mRuntimeError\u001b[0m\n",
      "\u001b[34m: Command '['neuron_parallel_compile', './fine_tuning_trn1.sh']' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:36:29,191 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:36:29,191 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-18 21:36:29,191 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-03-18 21:36:29,191 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"raise SignalException(f\"\n",
      " Process {os.getpid()} got signal: {sigval}\", sigval=\n",
      " sigval)\n",
      " torch.distributed.elastic.multiprocessing.api.SignalException: Process 2\n",
      " 80 got signal: 15\n",
      " 2024-03-18 21:36:28.000827:  274  ERROR ||NEURON_PARALLEL_COMPILE||: There was an error in the training script.\n",
      " raise RuntimeError(e)\n",
      " RuntimeError\n",
      " Command '['neuron_parallel_compile', './fine_tuning_trn1.sh']' returned non-zero exit status 1.\"\u001b[0m\n",
      "\u001b[34mCommand \"/usr/local/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.95 --append_eod False --constant_steps 0 --global_train_batch_size 256 --layer_norm_epilson 1e-05 --learning_rate 0.01 --lr_scheduler_type CosineAnnealing --max_input_length 2048 --max_steps 10 --min_learning_rate 1e-06 --mixed_precision True --per_device_train_batch_size 1 --pipeline_parallel_degree 1 --preprocessing_num_workers None --tensor_parallel_degree 8 --warmup_steps 10 --weight_decay 0.1\"\u001b[0m\n",
      "\u001b[34m2024-03-18 21:36:29,191 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-03-18 21:38:35 Uploading - Uploading generated training model\n",
      "2024-03-18 21:39:21 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job meta-textgenerationneuron-llama-2-7b-2024-03-18-21-26-15-153: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise SignalException(f\"\n Process {os.getpid()} got signal: {sigval}\", sigval=\n sigval)\n torch.distributed.elastic.multiprocessing.api.SignalException: Process 2\n 80 got signal: 15\n 2024-03-18 21:36:28.000827:  274  ERROR ||NEURON_PARALLEL_COMPILE||: There was an error in the training script.\n raise RuntimeError(e)\n RuntimeError\n Command '['neuron_parallel_compile', './fine_tuning_trn1.sh']' returned non-zero exit status 1.\"\nCommand \"/usr/local/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.95 --append_eod False --constant_steps 0 --global_train_batch_size 256 --layer_norm_epilson 1e-05 --learning_rate 0.01 --lr_scheduler_type CosineAnnealing --max_input_length 2048 --max_steps 10 --min_learning_rate 1e-06 --mixed_precision True --per_device_train_batch_size 1 --pipeline_parallel_degree 1 --preprocessing_num_workers None --tensor_parallel_degree 8 --warmup_steps 10 --weight_decay 0.1\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjumpstart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JumpStartEstimator\n\u001b[1;32m      4\u001b[0m estimator \u001b[38;5;241m=\u001b[39m JumpStartEstimator(\n\u001b[1;32m      5\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      6\u001b[0m     model_version\u001b[38;5;241m=\u001b[39mmodel_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# if not specified, default `ml.trn1.32xlarge` will be used.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_location\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/estimator.py:669\u001b[0m, in \u001b[0;36mJumpStartEstimator.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start training job by calling base ``Estimator`` class ``fit`` method.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03mAny field set to ``None`` does not get passed to the parent class method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m        (Default: None).\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    655\u001b[0m estimator_fit_kwargs \u001b[38;5;241m=\u001b[39m get_fit_kwargs(\n\u001b[1;32m    656\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    657\u001b[0m     model_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session,\n\u001b[1;32m    667\u001b[0m )\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJumpStartEstimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mestimator_fit_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_kwargs_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1341\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2680\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5766\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5747\u001b[0m \n\u001b[1;32m   5748\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5764\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5766\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:7995\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7992\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   7994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 7995\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7996\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   7997\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:8048\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8044\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8045\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8046\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8047\u001b[0m     )\n\u001b[0;32m-> 8048\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8049\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8050\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8051\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8052\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job meta-textgenerationneuron-llama-2-7b-2024-03-18-21-26-15-153: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise SignalException(f\"\n Process {os.getpid()} got signal: {sigval}\", sigval=\n sigval)\n torch.distributed.elastic.multiprocessing.api.SignalException: Process 2\n 80 got signal: 15\n 2024-03-18 21:36:28.000827:  274  ERROR ||NEURON_PARALLEL_COMPILE||: There was an error in the training script.\n raise RuntimeError(e)\n RuntimeError\n Command '['neuron_parallel_compile', './fine_tuning_trn1.sh']' returned non-zero exit status 1.\"\nCommand \"/usr/local/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.95 --append_eod False --constant_steps 0 --global_train_batch_size 256 --layer_norm_epilson 1e-05 --learning_rate 0.01 --lr_scheduler_type CosineAnnealing --max_input_length 2048 --max_steps 10 --min_learning_rate 1e-06 --mixed_precision True --per_device_train_batch_size 1 --pipeline_parallel_degree 1 --preprocessing_num_workers None --tensor_parallel_degree 8 --warmup_steps 10 --weight_decay 0.1\", exit code: 1"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\"\n",
    "    },  # please change `accept_eula` to be `true` to accept EULA.\n",
    "    instance_type=\"ml.trn1.2xlarge\", \n",
    "    # if not specified, default `ml.trn1.32xlarge` will be used.\n",
    ")\n",
    "\n",
    "estimator.fit({\"train\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3889d9-1567-41ad-9375-fb738db629fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Studio Kernel idle issue:  If your studio kernel goes idle and you lose reference to the estimator object, please see section [4. Studio Kernel Dead/Creating JumpStart Model from the training Job](#4.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy the fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01e892-a70e-487b-989f-c1bbe5caba23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_inference = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt_inference.format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    pretrained_response = pretrained_predictor.predict(payload)\n",
    "    responses_before_finetuning.append(pretrained_response[\"generated_text\"])\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    responses_after_finetuning.append(finetuned_response[\"generated_text\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db86872-cca4-4ec5-8574-6876668ebf12",
   "metadata": {},
   "source": [
    "### 2. Use text file as input to fine-tune LLaMA-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgenerationneuron-llama-2-7b\" #or  \"meta-textgenerationneuron-llama-2-13b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"false\"})\n",
    "# estimator.set_hyperparameters(max_steps=30)\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. Default: 2048.\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 6e-6.\n",
    "- min_learning_rate: The learning rate at the last step of learning rate scheduler 'CosineAnnealing'. Default: 1e-06.\n",
    "- global_train_batch_size: The global batch size for training. Based on global_train_batch_size, the gradient accumulation is calculated as global_train_batch_size / (data_parallel_degree * per_device_train_batch_size), where data_parallel_degree is calculated as total number of neuron cores / (tensor_parallel_degree * pipeline_parallel_degree). Default: 256.\n",
    "- per_device_train_batch_size: The batch size per Neuron core for training. Default: 1\n",
    "- layer_norm_epilson: During layer normalization, a value added to the denominator for numerical stability. See [documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). Default: 0.00001.\n",
    "- preprocessing_num_workers: The number of processors to use for the preprocessing. If None, all of workers (number of vCPUs) are used for preprocessing. Default: \"None\"\n",
    "- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in `AdamW` optimizer. Default: 0.1.\n",
    "- lr_scheduler_type: Learning rate scheduler type. Default: 'CosineAnnealing' (currently we only support 'CosineAnnealing' scheduler type).\n",
    "- warmup_steps: Linear warmup over warmup steps. Default: 10.\n",
    "- constant_steps: The number of steps for learning rate to be constant after warmup_steps in 'CosineAnnealing' scheduler type. Default: 0.\n",
    "- adam_beta1: The beta1 hyperparameter (exponential decay rate for the first moment estimates) for the AdamW optimizer. Default: 0.9.\n",
    "- adam_beta2: The beta2 hyperparameter (exponential decay rate for the first moment estimates) for the AdamW optimizer. Default: 0.95.\n",
    "- mixed_precision: Whether to use mixed precision. If mixed_precision to be 'True', it means that master weights and optimizer states are stored in fp32, and model weights are saved in bf16. For details, see [reference](https://arxiv.org/pdf/1710.03740.pdf). Default: 'True'.\n",
    "- tensor_parallel_degree: The number of neuron cores which specific model weights, gradients, and optimizer states are split across. For details, see [reference](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html). Default: \"8\" (currently we only support parallel degree as 8).\n",
    "- pipeline_parallel_degree: The number of neuron cores which the layers of a model are partitioned across. Default: \"1\" (currently we only support \"1\" for LLaMA-2 7B and \"4\" for LLaMA-2 13B).\n",
    "- append_eod: Whether to append an `<eod>` token to the end of each example. By setting it to 'True', the fine-tuned model tends to generate succinct output. Default: 'False'.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### 4. Studio Kernel goes idle/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Training job may take several hours due to setting of hyperparameters and the studio kernel may be in idle stage during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9708003",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|aws-trainium-inferentia-finetuning-deployment|llama-2-trainium-inferentia-finetuning-deployment.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
